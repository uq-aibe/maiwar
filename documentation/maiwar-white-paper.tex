\documentclass[12pt,a4paper,twoside, draft]{article}
   % \documentclass[12pt]{article}
\listfiles
\usepackage{./preamble-files/paper-preamble}
\makeatletter
\renewcommand\maketitle
  {\begin{center}\mdseries\large
    {\@title}%
    \par\medskip\medskip
    {\normalsize\@author}%
    \par\medskip\medskip\normalfont
    \begin{small}
\emph{Australian Institute for Business and Economics}
\end{small}
\par
\begin{small}
\emph{University of Queensland}
\end{small}
   \end{center}
  }
  \makeatother
  \usepackage{fancyhdr}
  \fancyhf{}
\fancyhead[LE,RO]{\small\thepage}
\fancyhead[CO]{\small\scshape MAIWAR}% odd page header 
\fancyhead[CE]{\small\scshape P.H. O'Callaghan}
\fancyfoot[L,R,C]{}
\renewcommand{\headrulewidth}{0pt}% disable the underline of the header part
\title{
   \textsc{
MAIWAR (Modelling Australian Industry with Weak-solutions, AMPL and Regions)
}
\footnote{
      This work is part of the Sustainability, Modelling and Regional
      Transition (SMaRT) project.
      Thanks to John Mangan and Brent Ritchie for winning financial support for
      the SMaRT project and to the University of Queensland for this support
      (Research Support Package - Round 3).
      I thank Josh Aberdeen, Patrick Duenow, Cameron Gordon, John Mangan and
      Tina Rampino for many conversations and active contributions to the
      creation of the MAIWAR model.
      I thank Alex Yelkhovski for valuable conversations at an early stage. 
      Last, but certainly not least, I thank the members of the SMaRT project
      panel: Hurriyet Babacan, Bego\~{n}a Dominguez, Flavio Menezes, Alicia
      Rambaldi and Martie-Louise Verreynne.
      This version has time stamp \currenttime~(AEST), \today. The most recent
      version (as well as past versions) can be found at
      \url{https://github.com/uq-aibe/maiwar/blob/main/documentation/maiwar-white-paper.pdf} .}}
\author{\large\textsc{By Patrick H. O'Callaghan}
   \footnote{Email address
    \href{mailto:p.ocallaghan@uq.edu.au}{\texttt{p.ocallaghan@uq.edu.au}} and
    ORCID iD \href{http://orcid.org/0000-0003-0606-5465}{
      \texttt{0000-0003-0606-5465}} }}


  \date{}
    
  \begin{document}
  \maketitle

  \pagestyle{fancy}
\renewcommand{\abstractname}{\vspace{-\baselineskip}} \thispagestyle{plain}
%\input{frontmatter}
%\maketitle
%\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
 % abstract

\begin{abstract}%\resetlinenumber
  
  Policy makers make decisions regarding medium term policies where capital is
  evolving.
  They do so in the face of uncertainty.
  Existing Computable General Equilibrium models do not adequately model
  such intertemporal behaviour.
  Macroeconomic models typically address this concern, but
  rely on local approximations around the non-stochastic steady-state
  (where capital is no longer evolving). Ergodic-set methods are similarly
  suited to the long-term whereas grid-based value-function iteration is
  unstable or slow in multi-sectoral settings.
  
  In this white paper, we extend a simple, yet powerful, method (of Cai and
  Judd) to the multisectoral setting. Along uncertain paths through time,
  agents look ahead multiple periods and choose policies that are optimal the
  certainty-equivalent sense. 
  The model focuses on the medium term and the solution we derive is
  \emph{weak}: characterised by an empirical distribution on the set of
  paths (unobservable strong solutions).
  Similar to structural econometric models, the key measure of accuracy is that
  the mean path satisfies the Intertemporal Euler equation to within a 1\%
  margin of error.
\end{abstract}
\setlength{\epigraphwidth}{11.5cm}
\epigraph{
   From the past, the present acts prudently, lest it spoil future action.
  }{\emph{Titian:  Allegory of Prudence}
}

%==============================================================================
\section{Introduction}\label{sec-introduction}
%==============================================================================
Experience is the basis of prediction.  Yet outside of stylised settings, even
the most experienced forecasters do not claim  access to ``the full
model''.
In the methodology below, we will argue that, for incomplete model
specifications, weak solutions, that is ones that explicitly uncertainty of
various kinds and are characterised by an empirical probability distribution
are more suitable.
Yet this kind of solution is absent from the literature: certainly from
the Computational General Equilibrium (CGE) literature, and the related
multi-sectoral macroeconomic literature.
The weak solution we derive is implicit in \citet{CJ}: though they do not
consider the case of multi-sectoral investment flows that is central to MAIWAR.

Beyond the short-term, economic theory provides the lens through which
policy-makers peer into the future. 
In medium term, the economy is unlikely to reach its steady state. 
Moreover, in models with multiple sectors recent research has highlighted
the importance of nonlinear effects.
For such problems, first-order steady-state approximations are ill-suited.
Yet this is the solution method that is most commonly applied today.

Computable General Equilibrium (CGE)
models are often used to provide insights into the regional and sectoral
impacts of current policies and shocks that propagate through the economy's
production networks over time.
Models that assume steady-state approximations are better suited to
long-term policy questions precisely because steady-state equilibria are a
special kind of equilibrium that may take many years to arrive.
Yet even the impact of longer-term policies, such as 2050 net-zero emissions
targets should involve a careful analysis of the short-to-medium term.
For a policy that is unsustainable or sub-optimal in the interim is liable to
fail altogether.

\emph{At this point, this white paper only contains a draft methodology.}

%==============================================================================
\section{Methodology}\label{sec-methodology}
%==============================================================================
%For the benefit of the wider community, we begin with methodology without any
%mathematics. All emphasized terms are formally defined in the full methodology
%that then follows.
%\subsection{Non-technical methodology}
The goal of our method is to generate statistical information about the path
that key regional variables such as gross value added, employment, consumption
and investment will take over the next few years.
In particular, we combine current and past data with the structure of economic
theory to generate an \emph{empirical likelihood}
\citet{Owen-Empirical_likelihood} on the set of uncertain paths through time.
This is what we refer to as a \emph{weak solution}.
The weak solution can then be used not only for providing policy guidance as is
the standard approach to CGE modelling, but also for quantifying the
uncertainty and for measuring the accuracy of the model itself.
For example, we can check that the mean of the empirical likelihood
satisfies an optimality criteria such as the intertemporal Euler equation. 

Before turning to a more detailed example with uncertainty, we explain the
model under certainty.
\section{A canonical example without uncertainty}\label{sec-certainty}
Under certainty the solution is a unique path through time.
In other words, the decision tree $\mc T$ simplifies to a chain or
sequence of decision nodes.\footnote{
   Formally, for each $n$ in $\mc T$, there is a unique $n^{\mplus}$ such that
   $(n, n^{\mplus})$ belongs to $\mc E$.
}

\section{The normalised Euler error}
We now derive the normalised Euler error for the example in
\cref{sec-certainty}.



\section{Uncertainty}
As in structural econometric modelling
\citep{Reiss_Wolak-Structural_econometrics} it is important to distinguish the
following kinds of uncertainty:
\begin{enumerate}[I]
   \item\label{aue} agent uncertainty about the economic environment;
   \item\label{aub} optimization errors / behaviour of economic agents;
   \item\label{rum} researcher measurement and approximation errors.
\end{enumerate}
%==============================================================================
\subsection{Type \ref{aue} uncertainty}
%==============================================================================
In MAIWAR, type \ref{aue} uncertainty is captured using a decision tree.
A decision tree $\mc T$ is a pair $(\mc N, \mc E)$ of nodes and directed
arcs/edges connecting future nodes to preceding ones.
A \emph{node} $n$ is where agents make decisions.
An \emph{edge} $e$ is a pair $(n, n^{\mplus})$ of nodes that are connected in
the sense that $n$ immediately precedes $n^{\mplus}$.
Uncertainty arises when there is more than one edge emanating from some node
$n$.\footnote{
   Since $\mc T$ is a tree, for every $e = (n , n^{\mplus}) \in \mc E$ if 
   $e' = (n', n^{\mplus}) \in \mc E$, then $n' = n$.
}
Uncertainty at node $n$ unfolds along edge $e$ to reveal the subsequent
decision node.
A \emph{path} through time consists of a maximal sequence of connected nodes.
That is sequences that start at the (unique) root node $\mathfrak{0}$ of the
tree and end on a leaf (a node with no explicitly modelled future times).
Let $\mc P$ denote the set of paths.

\begin{example}[Irreversible risk]\label{eg-irreversible}
   In our canonical example, where the focus is on net-zero emissions in the
   year 2050, we identify the root with the current year, \eg~2022.
   A leaf is then a state of the world at time 2050.
   Each year between now and 2049, there is a small but positive probability of
   a one-off, permanent productivity shock to output.
   The following paths characterise the decision tree:
   \begin{enumerate}[1]
      \item the path where the shock happens between 2022 and 2023;
      \item the path where the shock happens between 2023 and 2024;
      \item[] \dots
      \item[28] the path where the shock happens between 2049 and 2050.
   \end{enumerate}
   Each path identifies a distinct state of the world.
   Although this stochastic process is as simple as flipping a coin
   each year and the resulting state space consists of just 28 paths,
   the irreversible nature of the shock yields a
   \emph{nonstationary} (\ie~time inhomogeneous) economy with a nonunique
   steady state.
\end{example}

%==============================================================================
\paragraph{The objective function at each node}
%==============================================================================
At each decision node $n$ in $\mc N$, agents solve a forward-looking
optimization problem of the form that we now describe.
Let $T = 0, 1, \dots$ denote the set of look-ahead times and, as a matter of
expedience, let $\countof T \equiv T$.
The larger the set $T$ the more rational the agents in the economy: a point we
discuss further in section \ref{sec-approximation}.
In our canonical example and algorithm, we assume
a representative agent maximises the social welfare of the economy subject to
constraints.
(Recasting the model in terms of equilibrium equations is straightforward:
see \citet[Algorithm 2]{CJ}.)

The objective of the social planner at node $n$ is to choose the action $a$
that maximises the following forward-looking expression:
\begin{align}\label{eq-objective}
\hskip-3pt   \textup{obj}_{n}(a_{n}, \omega_{n}) \defeq
   \textup{u}_{n + 0}(a_{n + 0}, \omega_{n + 0})
    + \mbb{E}_{n}\left\{
      \textup{u}_{n + 1}(a_{n + 1}, \omega_{n + 1}) + \cdots
      + \textup{V}_{n + T}(\omega_{n + T})
    \right\}
  %\\
  %\textup{s.t.} & \quad \omega_{n + t+1}
  %  = g_{n + t}(\omega_{n + t}, a_{n + t}),
  %    \quad t = 0, \dots, T
  %\\
  %  & \quad f_{n + t}(\omega_{n + t}, a_{n + t}) \geq 0, \quad t = 0, \dots, T.
\end{align}
We now explain the terms in expression \eqref{eq-objective} in detail.
First, $\textup{V}_{n}$ is the value function at node $n$ and
$\textup{V}_{n + T}$ is the \emph{terminal} or \emph{continuation value} of the
optimization problem.
The \emph{reward}/\emph{utility} function $\textup{u}_{n + t}$ extends
$0 \leq t < T$ periods ahead from node $n$.
The conditional expectation $\mbb{E}_{n}$ is defined on the set of functions
that are measurable with respect to the information available to the agent at 
node $n$.
\footnote{
   All these functions are scalar-valued.
}
The state $\omega_{n + t} $ is a pair of variables consisting  of capital
$\mathbf{kap}_{n + t}$ and the productivity shock term
\begin{equation}
   \textup{shock}_{n + t} = \left\{
      \begin{array}{ll}
         0.9 & \textup{if the shock has happened}\\
         1 & \textup{otherwise.}
      \end{array}\right.
\end{equation}
We return to discuss capital in more detail below.
The shock $\textup{shock}$ is therefore macroeconomic in the sense that
its effect is uniform across regions $R$ and sectors $J$ in the economy.
A more general shock will yield a larger state space and corresponding set of
paths.\footnote{
   This simplification is purely for expositional purposes.
   In our code, this and other parameters that appear below are allowed to vary 
   across regions and sectors.
}

Finally, in \eqref{eq-objective}, action $a_{n + t}$ is often referred to as
the (set of) control variable(s).
In our canonical example, it is a quadruple consisting of future consumption
$\mathbf{con}_{n + t}$, labour $\mathbf{lab}_{n + t}$, investment
$\textup{inv}_{n + t}$ and savings $\mathbf{inv}_{n + t}$.
Note that $\mathbf{con}_{r, n + t}$, $\mathbf{lab}_{r, n + t}$ and
$\textup{inv}_{r, n + t}$ are vectors on $J$ indexed by region.
Similarly, we view $\mathbf{inv}_{r, n + t}$ as the $J\times J$ matrix-valued
function of flows between regions that we define in \eqref{eq-ces-inv} (also
indexed by region).
It is this latter matrix of \emph{dynamic} flows that significantly increases
the dimension of the problem: for 20 sectors, there are 400 flows; for 100
sectors, there are 10,000.

We now turn to the constraints, beginning with the transition laws/dynamics for
capital before turning to feasibility constraints such as market clearing.
%==============================================================================
\paragraph{Constraints at each node: transition dynamics}
%==============================================================================
Capital $\mathbf{kap}_{n + t}$ is a positive vector with values
$\R ^ {R\times J}$ that evolves according to the standard equation for each node
$n$ and time $t$.
Since the depreciation rate $\delta_{j}$ may depend on sector, we express this
as a scalar-valued equation:
\begin{equation}\label{eq-kapital}
   \textup{kap}_{r, j, n + t + 1} =
     (1 - \delta_{j}) \cdot \mathbf{kap}_{r, j, n + t}
       + \textup{inv}_{r, j, n + t}\,.\footnote{
  We present all \emph{basic variables} in \textbf{bold} font.
  Basic, in this setting, means that the variable cannot be explicitly
  written as a function of the other variables without recourse to
  first-order or second-order conditions of the optimization (or equilibrium)
  problem.
  In this case, capital at $n + t + 1$ can be written as function of capital at
  $n + t$ and investment at $n + t$.
}
\end{equation}
Note that \eqref{eq-kapital} is a stochastic difference equation.
This is because, for every $t$ in $T$, the set $N_{t}$ of nodes that are
reachable in $t$ steps typically contains more than one node.
In \eqref{eq-kapital}, $\textup{inv}_{r, j, n + t}$ is an aggregator over the
\emph{basic} fixed capital formation vector
$(\mathbf{inv}_{r, ij, n + t}: i \in J)$.
Note that fixed capital formation is more general than savings.
It includes, for instance, land reclaimed from forest or sea.
We assume that fixed capital formation and investment are related via a
constant elasticity of substitution (CES) function
\begin{equation}\label{eq-ces-inv}
  \textup{inv}_{r, j, n + t} = A_{r, j}^{\textup{inv}}
    \cdot \left(\sum_{i \, \in \, J}
      \sigma_{i j} ^ {\textup{inv}} \cdot \mathbf{inv}_{r, i j,\, n + t}
        ^ {\,\rho_{j} ^ {\textup{inv}}}
    \right) ^ {\kappa_{j} ^ \textup{inv} / \rho_{j} ^ \textup{inv}}.
\end{equation}
As usual, $\rho_{r, j}^\textup{inv}
  = (\varepsilon_{r, j}^\textup{inv} - 1) / \varepsilon_{r, j}^\textup{inv}$,
where the elasticity of substitution $\varepsilon_{r, j} ^ \textup{inv}$ is
typically $ 0.1$.
Thus, $\rho_{r, j}^\textup{inv} \approx -9$.
Next, $(\sigma_{r, ij}^\textup{inv}: i \in J)$
%= (\hat{\sigma}_{r, ij}^\textup{inv}) ^ {1 / \varepsilon_{r, j}^\textup{inv}}$
is a vector of share parameters we obtain by taking a column of intermediate
flows from a standard input-output table, normalising so that the column sum is
one and applying the exponent $1/\varepsilon_{r, j}^\textup{inv}$ to each
element.
Finally, for strict concavity of the relation
$\left(\mathbf{inv}_{r, ij, n + t} : i \in J\right)
  \mapsto \textup{inv}_{r, j, n + t}$ we assume the scale parameter 
$\kappa_j^\textup{inv}$ is slightly less than $1$.\footnote{
  In \citet[Theorem 2]{Kojic-Concavity} (for instance) it is shown that a
  necessary and sufficient condition for strict concavity is
  $0 < \kappa_{r, j}^\textup{inv} < 1$.
}

We capture the cost $\textup{adj}_{r, j, n + t}$ of adjusting capital
via the relation 
\begin{equation}
  \textup{adj}_{r, i, n + t}
    = A_{r, i} ^ \textup{adj} 
      \cdot \frac{(\textup{kap}_{r, i, n + t + 1}
      - \mathbf{kap}_{r, i, n + t})^2}
        {\mathbf{kap}_{r, i}}.
\end{equation}
Some form of inertia makes for more realistic rates of convergence between
regions.
For each region and each sector, this cost is deducted from time $n + t$ output
to which we now turn.
%==============================================================================
\paragraph{The production function}
%==============================================================================
In addition to capital, labour is the other \emph{primary factor of
production.}
Labour resources also vary across regions, sectors and time.
To simplify this exposition, we avoid notions of human capital.

In addition to primary factors, we have intermediate inputs.
As with investment, we specify intermediate inputs to production via a CES
aggregator $f_{r, j}^{\textup{med}}: \nnreal^{J} \rightarrow
\nnreal$.\footnote{
  The superscript in $f_{r, j}^{\cdot}$ indexes the variety of CES
  aggregators in the model.
  Each function $f_{r, j}^{\cdot}$ is characterised by a corresponding
  set of parameters
  $A_{r, j}^{\cdot}, (\sigma_{r, ij}^{\cdot}: i \in J),
  \rho_{r, j}^{\cdot}, \kappa_{r, j}^{\cdot} $ and
  $\varepsilon_{r, j}^{\cdot}$ as per \cref{eq-ces-inv}.
}
Thus,
\begin{equation}\label{eq-ces-int}
  \textup{med}_{r, j, n + t} = A_{r, j, n + t} ^ \textup{med}
    \cdot f_{r, j} ^ \textup{med}(
      \mathbf{med}_{r, ij, n + t}: i \in J
      )
\end{equation}
where, for each $r, j, n$ and $t$, $(\textbf{med}_{r,ij,n + t} : i \in J)$
corresponds to the $j$th column of a standard input-output matrix of
intermediate flows.
%Long--Plosser flows (between sectors and across regions) do not distinguish
%between flows of goods such as flour or water that are sometimes viewed as
%purely intermediate and flows such as machinery that are sometimes viewed pure
%capital.
%In \citet{Long_Plosser-Real_business_cycles}, inventories of flour are a form
%of capital and current usage of machinery is an intermediate input.
%Although this assumption is not essential to the methodology, it is reasonable
%given the available data.
%For although standard input-output tables report investment as separate from
%intermediate inputs, this is on the basis of accounting as opposed to economic
%principles.
%Moreover, recent attempts \citep{Elazaar_Raymond-Network} to estimate
%production networks using micro data (BLADE and GST data) do not distinguish
%between these two categories of flows.
%It is worth pointing out that although intermediate production is well-modelled
%in most CGE models, savings and investment do not contribute to future 
%utility in any meaningful way \citep{Hosoe_et_al-CGE_textbook}.


For every $r$ in $R$, $i$ in $J$, let
$f_{r, i, n + t}^{\textup{out}}: \nnreal^{3} \rightarrow \nnreal$ be a standard
CES function of capital, labour and the intermediate-input bundle respectively:
\begin{equation}
	\textup{out}_{r, i, n + t}
    = A_{r, i, n + t} ^ \textup{out} \cdot f_{r, i}
      ^ \textup{\,out}(
        \mathbf{kap}_{r, i, n + t},
        \mathbf{lab}_{r, i, n + t},
        \textup{med}_{r, i, n + t}
        )
\end{equation}
%\begin{equation}
%  f_{r, i, n + t} ^ {\textup{out}} (x, y, z)
%	  = \left(
%      \alpha_{r, i} ^ {\textup{out},x} \cdot x ^ {\,\rho ^ \textup{out}}
%      + \alpha_{r, i} ^ {\textup{out},y} \cdot y ^ {\,\rho ^ \textup{out}}
%      + \alpha_{r, i} ^ {\textup{out},z} \cdot z ^ {\,\rho ^ \textup{out}}
%    \right) ^ {\, \frac{\kappa ^ {\textup{out}}}{\rho ^ \textup{out}}}
%     .\footnote{
%        As in \cref{eq-ces-inv}, the scale parameter $\kappa$ is
%        approximately equal to, but less than one to ensure strict concavity.
%        The $\alpha_{r, i}$ capture the relative importance of capital and
%        labour in each region-sector pair.
%     }
%\end{equation}
%==============================================================================
\paragraph{Constraints at each node: market clearing}
%==============================================================================
In our canonical model, there is a single market for each sector.
This implies free trade between regions.
Finally, the key feasibility condition is that the following expression (where
we have suppressed reference to $n + t$), for total production minus total
usage, is nonnegative for every $i$ in $J$:
\begin{equation}
  \sum_{r \,\in\, R} \left\{
  \textup{out}_{r, i}
  - \mathbf{con}_{r, i}
  - \sum_{j \,\in\, J}
      \left\{\mathbf{inv}_{r, ij} + \mathbf{med}_{r, ij}\right\}
  - \textup{adj}_{r, i} \right\}.
\end{equation}
The market clearing property itself then follows by virtue of the fact that
the reward function $u_{n + t}$ is strictly increasing in the
consumption of every good.
\paragraph{Reward and Terminal Value functions at each node}
Beyond strictly increasing, we take the utility function
\[
u_{r, n + t}: \nnreal^{J \times J} \rightarrow \R, \quad
  (\mathbf{con}_{r, n + t}, \mathbf{lab}_{r, n + t})
    \mapsto \textup{utl}_{r, n + t},
\]
for each region, node and time
to be strictly concave in consumption and labour.
Similar to both \citet{Atalay-Sectoral_shocks} and \citet{CJ}, we have
\begin{equation}
  \textup{utl}_{r, n + t}
    = \beta^{t}
%%      \cdot \left(
%%        \sum_{i \,\in\, J} \sigma_{r, i}^\textup{utl}
%%          \cdot \mathbf{con}_{r, i, n + t} ^ {\,\rho_{r}^\textup{utl}}
%%        \right) ^ {\kappa_{r}^{\textup{utl}} /\rho_{r}^{\textup{utl}}}
      \cdot \left(
        A_{r}^\textup{utl}
          \cdot f_{r}^{\,\textup{utl}}(\mathbf{con}_{r, i, n + t}: i \in J)
        - B_{r}^\textup{utl}
          \cdot \left(\sum_{i \, \in \, J} \mathbf{lab}_{r, i, n + t}\right)
          ^ {\eta_{r, i}}
      \right)
\end{equation}
where $0 < \beta < 1$ is the parameter that describes how agents discount
future consumption and work;
$\eta_{r, i} = 1 + 1 / \varepsilon_{r, i}^\textup{lab}$
is the parameter that relates the Frisch elasticity
$\varepsilon_{r, i}^\textup{lab}$  of labour supply to utility.
If the scale parameter $\kappa_{r, n + t}^\textup{utl}$ of 
$f_{r, n + t}^\textup{utl}$ 
$\textup{u}_{r, n + t}$ is strictly concave.
The weighted sum across regions (with weights determined by population) of such
regional reward functions yields the ``global''/``whole-of-economy'' reward
functions $\textup{u}_{n + t}$ we find in the objective function
\eqref{eq-objective} each node.
This, together with standard assumptions
\citep[Theorem 12.2.12]{Stachurski-Economic_dynamics}, is sufficient for strict
concavity of the terminal value function $\textup{V}_{n + T}$ in the state
vector $\omega_{n + t}$.
Together with the structure we have imposed on the transition laws and
feasibility constraints, the social welfare Lagrangian $\mc L_{n}$ is
strictly concave function of the control vector $a_{n}$ for every $n$ in
$\mc N$.
\footnote{
Recall that the Lagrangian is the sum of the objective and each of the
constraints scaled by the \emph{dual} variables (also known as Lagrange
multipliers).
}
\begin{remark*}
As usual, the Lagrange multipliers of market clearing constraints are also the
competitive market prices.
The AMPL language has features for constraining prices where necessary.
\end{remark*}
\paragraph{Well-posedness of the optimization problem at each node}
The above conditions ensure that the solving for the general equilibrium of the
economy is equivalent to solving a strictly concave program at each node.
This implies that the equilibrium at each node is unique.
The first-order (necessary) conditions for are also sufficient and we can
tackle the problem with local (nonlinear optimization) solvers such as Conopt
or Ipopt which are many times faster than global solvers such as Baron.

\paragraph{Constructing the empirical distribution on paths}
Let us first define the strong solution we obtain from the above node-specific
optimization problems.
For a given decision tree $\mc T = (\mc N, \mc E)$ and corresponding set of
paths $\mc P$, let
\[
   \textup{StrongSol}(\mc P) \defeq \left\{
      (\omega^{*}_{p}, a^{*}_{p}) : p \in \mc P
   \right\}
\]
where the superscript $*$ denotes the optimal state-policy values
for each decision node $n$ along a given path $p$.
Note that $\textup{StrongSol}(\mc P)$ only contains the realised initial
state-policy values since, for each $n$, the look-ahead values
$\left\{(\omega^{*}_{n + t}, a^{*}_{n + t}) : t > 0 \right\}$ are only used to
formulate the plan at node $n$.
This notion of solution is strong in the sense that, assuming the model is
fully-specified, it tells us precisely what will happen once the economy
reaches a given node.
Moreover, in simple cases, such as example \ref{eg-irreversible} where there
are just $28$ paths, $\textup{StrongSol}(\mc P)$ can also be viewed as a
complete contingent plan of action and future capital.
Another sense in which the solution is strong when $\countof \mc P$
is small is that we are able to observe the entire sample space: we are able to
observe the true distribution of optimal paths through time.

\paragraph{Weak solutions}
Beyond simple examples such as \cref{eg-irreversible}, we can only hope to
partially observe the sample space.
The set of optimal solution paths that we have derived is then a subsample and,
since our collection of nodes is incomplete, we can no longer refer to our
solution as strong.
The only sense in which we can speak of a solution is in terms of the empirical
distribution over paths. 
From this we can estimate the expected path, the variance, and so on.
We now formalise this weaker notion of solution.

Let $\mathfrak{0}$ denote the root node of the decision tree.
Recalling that we assume the decision tree has paths of uniform length, let $S$
denote the set of \emph{path times}: for each $s$ in $S$ the node $p_s$ is $s$
steps into the future from $\mathfrak{0}$ along path $p$.
We begin by constructing the empirical distribution associated with
$\mathfrak{0} + s$ for each $s \in S$.
Let $N_s$ denote the set of nodes that are $s$ steps into the future from
$\mathfrak{0}$.\footnote{
   That is $N_{s} = \left\{ p_{s} \in \mc N : p \in \mc P\right\}$.
}

For any $s$ in $S$ note that $\mathbf{con}_{r, i, \mathfrak{0} + s}$ is a
scalar-valued random variable on $N_s$. 
Moreover, for each $n$ in $N_s$, let $\textup{CON}_{r, i, n}$ denote a
realisation of this random variable.
By realisation, we mean a strong solution to this particular variable.
For any of our policy or state variables $x_{\mathfrak{0}+s}$ let $X_n$ denote
the realisation associated with node $n$.
Then the empirical distribution generated by $\left\{X_n: n \in N_s\right\}$ is
the function
$F_{x_{s}} : \R \rightarrow [0, 1]$,
\begin{equation}\label{eq-empirical-dbn}
   F_{x_s}(z) = \frac{1}{\countof N_{s}}
   \cdot \sum_{n \,\in\, N_{s}} 1_{X_n \,\leq\, z}.
\end{equation}

The weak solution for a given policy or state variable $x$ is simply the
sequence $F_{x} = \left\{F_{x_{s}}: s \in S\right\}$ of empirical distributions.
$F_{x}$ is the empirical distribution of the path-valued random variable $x$.
For instance, $F_{\mathbf{con}_{r, i}}$ is the empirical distribution for the
path of consumption of sector-$i$ goods in region $r$.

In turn, the weak solution for the entire problem is the joint empirical
distribution $F_{\mathfrak{0}}$ we obtain by taking the product of the
marginals.
(This is the case because all these probability measures are push-forwards that
are dependent on the same source of uncertainty.)
\paragraph{Measuring accuracy in terms of the Euler error}
One normative measure of the accuracy of our weak solutions is the Euler error
that is associated with the (intertemporal) Euler equation.
This equation is a necessary condition for optimality of the stochastic dynamic
program \citet{Stokey_Lucas-Recursive}.
In the present setting the derivation of Euler equation is complicated by two
factors: the number of control variables exceeds the number of state variables;
and there is no explicit form for
To address this issue, for each $n$, we appeal to the envelope theorem.
In particular, we follow \citet{Gonzalez_Hernandez-Euler}
and derive an indirect utility function
$\textup{v}_{n}$ as a function of current and next-period states $\omega_{n}$
and $\omega_{n^{\mplus}}$.
\begin{align*}
   \textup{v}_{n}(\omega_{n}, \omega_{n^{\mplus}}) \defeq
   \max_{a_{n}}  \quad \textup{u}_{n}(\omega_{n}, a_{n}) \quad &
     \textup{subject to}\\
   \quad \omega_{n^{\mplus}} = \textup{tran}_{n}(\omega_{n}, a_{n}) \quad
   \textup{and} & \quad \textup{feas}_{n}(\omega_{n}, a_{n}) \geq 0
\end{align*}
where the transition and feasibility constraints are those we have described
above.
%As a matter of expedience, we follow the AMPL convention of identifying
%Lagrange multipliers with their corresponding constraint nomenclature.
%Thus $\textup{tran}_{s}$ and $\textup{feas}_{s}$ will denote the (random)
%Lagrange multipliers $s$ steps into the future from the root node
%$\mathfrak{0}$.
Then, via the envelope theorem, for models with exogenous uncertainty (where
actions do not affect transition probabilities), the Euler error associated
with step $s$ is
\begin{equation}
\frac{
  \beta \cdot \mathbb{E}_{\mathfrak{0}} \left\{
    \nabla_{\mathbf{kap}_{s+1}} \textup{v}_{s+1}(\omega_{s+1}, \omega_{s+2})
  \right\}
  }{\mathbb{E}_{\mathfrak{0}} \left\{
    \nabla_{\mathbf{kap}_{s+1}} \textup{v}_{s}(\omega_{s}, \omega_{s+1})
  \right\}
  } + 1
\end{equation}
It requires that the marginal loss (the denominator is negative) associated
with foregoing today's consumption (by increasing tomorrow's capital) is
balanced against the marginal benefit of increasing consumption tomorrow.
Note that the expectation $\mbb E_{\mathfrak{0}}$ in the expression for
Euler error is the empirical one we have derived as part of our weak solution.
In other words, our condition for accuracy demands that the empirical Euler
error is small.
As \citet{CJ} show for a wide variety of settings, the worst case of this error
across $s$ in $S$ is less than $1\%$.

We note that, since there is no explicit form for the indirect utility function
$\textup{v}$, we build on \citet{Scheidegger_Bilionis-Gaussian_processes} to
use gaussian processes to generate an approximation.
That is, we introduce uncertainty of type \ref{rum}.
This brings us to the next section.
%==============================================================================
\subsection{Types \ref{aub} and \ref{rum} uncertainty: approximations}
\label{sec-approximation}
%==============================================================================
In MAIWAR, weak solutions allow us to accommodate uncertainty of type \ref{aub}
and \ref{rum} without sacrificing theoretically appealing
solution properties such as the \emph{intertemporal Euler equation} a feature
that is absent from CGE models: see \cite{Dixon_Rimmer-Euler_CGE} for a recent
attempt to introduce this concept into the CoPS methodology.\footnote{
   This paper provided inspiration for our efforts.
}
Our agents (and modellers) strive to ensure that the marginal benefits of
current consumption against the marginal benefits of uncertain future
production.

Yet in order to arrive at a tractable and fast method for deriving a weak
solution, we will need to approximate.
By this we do not only mean the inevitable partial sampling of the set of
paths we have already highighted in the previous section, we mean:
\begin{enumerate}
\item approximating the terminal value function (see 
\citet{Bertsekas-Reinforcement_learning} for a detailed exposition of this
matter in the setting of reinforcement learning);
\item  by using certainty-equivalent projections (see \citet{CJ} and
   \citet{CJS-Certainty_equivalent} for a detailed exposition of this concept).
\end{enumerate}

Our first departure from canonical expected utility maximisation, where agents
form a complete contingent plans of action, is as follows.
At each decision node, agents solve a nonlinear program where errors
associated with future type \ref{aue} uncertain variables are replaced with
some convex combination of the best and worst case scenario.
Abundant evidence from the literature on behavioural economics shows
that agents adopt similar heuristic approaches to dealing with complex
decisions in the face of uncertainty.
The ``\emph{certainty equivalent}'' projection of \cite{CJ} is one such
heuristic. 
Aside from tractability, the inbuilt flexibility and scope for improvement in
aspect of the model is one reason for adopting the methodology of \cite{CJ}.
That is say, we view this, not only as an approximation, but also a reasonable
behavioural feature.

A second form of type \ref{aub} uncertainty arises because economic agents
struggle with long time horizons: indeed, isn't this why policy makers appeal 
to CGE models in the first place?
Evidence shows that agents focus their attention on the next few periods.
This motivates the finite look-ahead feature of our model.
Our agents focus on the near future as opposed to the far-flung steady states
at the heart of other approaches.

In MAIWAR there is a useful alignment between modeller's interests and the type
\ref{aub} uncertainties we have just outlined.
For although there are well-established methods for implementing large-scale
linear stochastic programs, the tools for large-scale nonlinear stochastic
programming are less evolved \citet{Rehfeldt-PETSc}.
Furthermore, although the time horizon of our agents is indefinite (and
typically modelled as infinite), most methods for dealing with such time
horizons assume stationarity or provide a linear approximation around the
nonstochastic steady state.
Nor is there an obvious way to model 2050 net-zero emissions without proper
modelling of the medium term.
Constraints that will bind in 28 years time, but have implications today are
poorly suited to steady-state approximations and standard policy/value
function iteration methods.
\footnote{
   In our experience, "global" value and policy function iteration methods
   are also slow and unstable in the setting of multi-sectoral models with
   medium-term constraints.
   See \citet{CJ} for a related discussion.
}

In summary, our agents solve finite sequences of nonlinear programs each with a
finite horizon and an approximation for the terminal value function. 
This is an approximation in the sense that normative agents solve nonlinear
stochastic programs each with an infinite horizon or the true terminal value
function.
We argue that our approximations are well-aligned with behaviour or agents in
practice.
Moreover, the weak solution that we derive explicitly accommodates our
approximation methods.
Finally, since this weak solution is an empirical distribution we can verify 
that the solution is accurate to within a $1\%$ Euler error approximation. 
The same measure is typical of ``global approaches''.
For instance, in the deep neural network approach of
\citet{Scheidegger_et_al-DEQN} the policy iteration process is successful once
the Euler error term converges to a sufficiently small number.
Our investigations of this approach yielded slow and unstable results.
The notion of a weak solution recognizes the fact that our modeller is
typically facing substantial type \ref{rum} uncertainty.
Parametric uncertainty is a feature of the model and a central part of the
solution.

\subsection{Computation and software}

The uncertainty we model is facilitated and expedited by the fact that we have
in place the software infrastructure to ensure that simple model sweeps are
easy to conduct on the local HPC cluster.
The unit for parallelisation is paths: each path is passed to a single core.
This means that most desktop computers can easily parallelise.
For example, consider our canonical example of irreversible risk: where there
are 28 paths (each of length 28 years), and assume 7 regions and 20 sectors.
For the functions we specify above, each step along a path takes $20$ seconds
on average.
Thus a path takes $28\times \frac{1}{3} \approx 9$ minutes to solve.
For a desktop with 4 available cores, we are able to assign 7 paths to each
core and complete the generate the full empirical distribution in
$7\times 9 \approx 1$ hour.

After an exhaustive search, we chose AMPL (A modelling language for
mathematical programming) \citet{Fourer_Gay_Kernighan-AMPL} as the language and 
ecosystem for communicating with solvers.
AMPL is used for pre-processing/pre-solving: the stage before the nonlinear
optimization problem is passed to the solver.
The goal of pre-solving is to ensure the solver stage is faster and more
reliable.
Pre-solving includes the generation of automatic, sparse derivatives as well as
domain/variable reduction.
AMPL syntax is much closer to standard mathematics relative to GAMS and we have
found it to produce faster total solve times than GAMS.\footnote{
\citet{CJ} find that GAMS is substantially faster than Matlab.
}
We spent a lot of time trying purely open source options in Python, Julia and
to lesser extent C++.
This experimentation also involved machine learning tools (based on
Scheidegger's work).
We found that, overall, combining the \citet{CJ} methodology with AMPL
pre-solving yields unrivalled modelling performance, flexibility and
reliability.


We have found three nonlinear solvers to be most useful.
Conopt which is proprietary and based on the reduced-gradient method (itself
based on the robust simplex method).
The open source Ipopt (pronounced ``eye-pea-opt"). This is celebrated
application of the interior point by \citet{Wachter_Biegler-Ipopt}.
Knitro which is tries a number of approaches and, unlike Conopt and Ipopt can
handle integer-valued variables. 
We have tried other solvers as well, but the above are outstanding, with Conopt
being the most robust.

Future improvements to our methodology relate to tuning MAIWAR model
parameters and improving on our existing framework for parallel computing.
In particular, to enable simple automated parallelisation even desktop
computers, we will use the Python API for AMPL along with nextflow.
Currently this process is manual and automatic tuning of parameters using
machine learning is  step in the development of MAIWAR.
Many of the tools that we worked on in the second half of last year (first six
months of SMaRT) will serve us well in this respect.

\subsection{Nonlinearity and (non)steady-state methods}

\citet{Atalay-Sectoral_shocks} estimates to be approximately $0.1$ so that
$\rho \approx -9$.
\\
Such approximations are even less appealing for nonlinear production networks
that arise when elasticities of substition are below one.
\\
Impressive recent work using Hamiltonian methods still fails to capture
the central economic theme of uncertainty.


\bibliographystyle{apalike} \bibliography{references}
\end{document}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
