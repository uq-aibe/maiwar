\documentclass[12pt,a4paper,twoside, draft]{article}
   % \documentclass[12pt]{article}
\listfiles
\usepackage{./preamble-files/paper-preamble}
\makeatletter
\renewcommand\maketitle
  {\begin{center}\mdseries\large
    {\@title}%
    \par\medskip\medskip
    {\normalsize\@author}%
    \par\medskip\medskip\normalfont
    \begin{small}
\emph{Australian Institute for Business and Economics}
\end{small}
\par
\begin{small}
\emph{University of Queensland}
\end{small}
   \end{center}
  }
  \makeatother
  \usepackage{fancyhdr}
  \fancyhf{}
\fancyhead[LE,RO]{\small\thepage}
\fancyhead[CO]{\small\scshape MAIWAR}% odd page header 
\fancyhead[CE]{\small\scshape P.H. O'Callaghan}
\fancyfoot[L,R,C]{}
\renewcommand{\headrulewidth}{0pt}% disable the underline of the header part
\title{
   \textsc{
MAIWAR (Modelling Australian Industry with Weak-solutions, AMPL and Regions)
}
\footnote{
      This work is part of the Sustainability, Modelling and Regional
      Transition (SMaRT) project.
      Thanks to John Mangan and Brent Ritchie for winning financial support for
      the SMaRT project and to the University of Queensland for this support
      (Research Support Package - Round 3).
      I thank Josh Aberdeen, Patrick Duenow, Cameron Gordon, John Mangan and
      Tina Rampino for many conversations and active contributions to the
      creation of the MAIWAR model.
      I thank Alex Yelkhovski for valuable conversations at an early stage. 
      Last, but certainly not least, I thank the members of the SMaRT project
      panel: Hurriyet Babacan, Bego\~{d}a Dominguez, Flavio Menezes, Alicia
      Rambaldi and Martie-Louise Verreynne.
      This version has time stamp \currenttime~(AEST), \today. The most recent
      version (as well as past versions) can be found at
      \url{https://github.com/uq-aibe/maiwar/blob/main/documentation/maiwar-white-paper.pdf} .}}
\author{\large\textsc{By Patrick H. O'Callaghan}
   \footnote{Email address
    \href{mailto:p.ocallaghan@uq.edu.au}{\texttt{p.ocallaghan@uq.edu.au}} and
    ORCID iD \href{http://orcid.org/0000-0003-0606-5465}{
      \texttt{0000-0003-0606-5465}} }}


  \date{}
    
  \begin{document}
  \maketitle

  \pagestyle{fancy}
\renewcommand{\abstractname}{\vspace{-\baselineskip}} \thispagestyle{plain}
%\input{frontmatter}
%\maketitle
%\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
 % abstract

\begin{abstract}%\resetlinenumber
  
  Policy makers make decisions regarding medium term policies where capital is
  evolving.
  They do so in the face of uncertainty.
  Existing Computable General Equilibrium models do not adequately model
  such intertemporal behaviour.
  Macroeconomic models typically address this concern, but
  rely on local approximations around the non-stochastic steady-state
  (where capital is no longer evolving). Ergodic-set methods are similarly
  suited to the long-term whereas grid-based value-function iteration is
  unstable or slow in multi-sectoral settings.
  
  In this white paper, we extend a simple, yet powerful, method (of Cai and
  Judd) to the multisectoral setting. Along uncertain paths through time,
  agents look ahead multiple periods and choose policies that are optimal the
  certainty-equivalent sense. 
  The model focuses on the medium term and the solution we derive is
  \emph{weak}: characterised by an empirical distribution on the set of
  paths (unobservable strong solutions).
  Similar to structural econometric models, the key measure of accuracy is that
  the mean path satisfies the Intertemporal Euler equation to within a 1\%
  margin of error.
\end{abstract}
\setlength{\epigraphwidth}{11.5cm}
\epigraph{
   From the past, the present acts prudently, lest it spoil future action.
  }{\emph{Titian:  Allegory of Prudence}
}

%==============================================================================
\section{Introduction}\label{sec-introduction}
%==============================================================================
Experience is the basis of prediction.  Yet outside of stylised settings, even
the most experienced forecasters do not claim  access to ``the full
model''.
In the methodology below, we will argue that, for incomplete model
specifications, weak solutions, that is ones that explicitly uncertainty of
various kinds and are characterised by an empirical probability distribution
are more suitable.
Yet this kind of solution is absent from the literature: certainly from
the Computational General Equilibrium (CGE) literature, and the related
multi-sectoral macroeconomic literature.
The weak solution we derive is implicit in \citet{CJ}: though they do not
consider the case of multi-sectoral investment flows that is central to MAIWAR.

Beyond the short-term, economic theory provides the lens through which
policy-makers peer into the future. 
In medium term, the economy is unlikely to reach its steady state. 
Moreovei, rn models with multiple sectors recent research has highlighted
the importance of nonlinear effects.
For such problems, first-order steady-state approximations are ill-suited.
Yet this is the solution method that is most commonly applied today.

Computable General Equilibrium (CGE)
models are often used to provide insights into the regional and sectoral
impacts of current policies and shocks that propagate through the economy's
production networks over time.
Models that assume steady-state approximations are better suited to
long-term policy questions precisely because steady-state equilibria are a
special kind of equilibrium that may take many years to arrive.
Yet even the impact of longer-term policies, such as 2050 net-zero emissions
targets should involve a careful analysis of the short-to-medium term.
For a policy that is unsustainable or sub-optimal in the interim is liable to
fail altogether.

\emph{At this point, this white paper only contains a draft methodology.}

%==============================================================================
\section{Methodology}\label{sec-methodology}
%==============================================================================
%For the benefit of the wider community, we begin with methodology without any
%mathematics. All emphasized terms are formally defined in the full methodology
%that then follows.
%\subsection{Non-technical methodology}
The goal of our method is to generate statistical information about the path
that key regional variables such as gross value added, employment, consumption
and investment will take over the next few years.
In particular, we combine current and past data with the structure of economic
theory to generate an \emph{empirical likelihood}
\citet{Owen-Empirical_likelihood} on the set of uncertain paths through time.
This is what we refer to as a \emph{weak solution}.
The weak solution can then be used not only for providing policy guidance as is
the standard approach to CGE modelling, but also for quantifying the
uncertainty and for measuring the accuracy of the model itself.
For example, we can check that the mean of the empirical likelihood
satisfies an optimality criteria such as the intertemporal Euler equation. 

Before turning to a more detailed example with uncertainty, we explain the
model under certainty.
%==============================================================================
\section{The model under certainty}\label{sec-certainty}
%==============================================================================
Certainty is characterised by a unique path through time.
Each step along this path is a node where economic decisions are made.
%Decisions regarding the resources that are to be consumed today and those
%that are to be invested in the future.
Let $\nodes$ denote the set of decision nodes or \emph{starting} nodes.
Under certainty, the unique path induces a natural ordering on
$\nodes$.\footnote{
  Formally, for each $\node$ in $\mc T$, there is a unique
  $\node^{\mplus}$ such that the \emph{edge} $\edge = (\node, \node^{\mplus})$
  belongs to the set $\edges$ of edges formed with members of $\nodes$.
}
At each $\node$ in $\nodes$, the economic agent(s) look ahead and make a plan
that extends into the future indexed by the set $T$ of look-ahead
times.\footnote{\citet{CJ} allow the set $T$ to vary across $\nodes$ whereas we
assume it is constant to simplify the exposition.}
The agent(s) formulate plans that ensure resources are optimally allocated
throughout $T$.

\begin{example}[2050 Net-zero carbon emissions target]
  Consider a government that wishes to acheive a net-zero carbon emissions
  target by 2050.
  Under certainty and some form of adjustment costs, it is natural to
  assume a gradual transition is optimal: one that spreads the discounted cost
  of transition to clean energy evenly across time.
  Our goal is to derive an estimate of key policy and state variables along
  this optimal path.
  That is to say, we seek a suitably accurate \emph{policy approximation}.
  
  In this setting, $\nodes$ consists only of the years  $2022, \dots, 2050 :$
  when the target is met.
  In contrast, $T$ might extend much further into the future \eg\
  $\sup T = \infty$.
  Or, since planning and computation is costly, agents might adopt some
  suitably small, finite horizon.
  For $T < \infty$, it is necessary to approximate the continuation value of
  the agents' optimisation problem.
  The most interesting case is where $T$ is equal to the number of years it
  takes to approximately reach the steady state of the model.
  This is the case where our approximation of the continuation
  value \emph{locally around the steady-state equilibrium of the model} is most
  natural.
  What distinguishes MAIWAR from most other approaches is that \emph{only} the
  continuation value is approximated in this way: the rest of the model, and in
  particular policy over the near-to-medium term, is modelled in full.
\end{example}

%==============================================================================
\paragraph{The objective function at each node}
%==============================================================================
%At each $\node$ in $\nodes$, agents solve a forward-looking
%optimization problem of the form that we now describe.
%Let $T = 0, 1, \dots$ denote the set of look-ahead times and,
As a minor expedience, let $T$ not only denote the set of look-ahead times, let
it also also denote its own cardinality $\countof T$.
The larger the set $T$, the more rational the agents in the economy: a point we
discuss further in section \ref{sec-approximation}.
%We assume
%a representative agent maximises the social welfare of the economy subject to
%constraints.
%(Recasting the model in terms of equilibrium equations is straightforward:
%see \citet[Algorithm 2]{CJ}.)
Under certainty, the agents at node $\node$ take the state
$\boldsymbol{\omega}_{\node}$ as given and choose the action
$\mathbf{a}_{\node}$ that maximises
the following forward-looking scalar-valued objective function:
\begin{align}\label{eq-objective} \hskip-3pt   
%  \textup{obj}_{\node}
  F_{\node}
    (\mathbf{a}_{\node}, \boldsymbol{\omega}_{\node}) \defeq
    \sum\left\{
      {u}_{\node, t}(\mathbf{a}_{\node, t}, \boldsymbol{\omega}_{\node, t})
      : 0 \leq t < T
    \right\}
    + {V}_{\node, T}(\boldsymbol{\omega}_{\node, T})
%
%    + \mbb{E}_{\node}\left\{
%     + \cdots
%     + {u}_{\node, T - 1}(\mathbf{a}_{\node, T - 1},
% \boldsymbol{\omega}_{\node, T - 1})
%    \right\}
  %\\
  %\textup{s.t.} & \quad \boldsymbol{\omega}_{\node, t+1}
  %  = g_{\node, t}(\boldsymbol{\omega}_{\node, t}, \mathbf{a}_{\node, t}),
  %    \quad t = 0, \dots, T
  %\\
  %  & \quad f_{\node, t}(\boldsymbol{\omega}_{\node, t}, \mathbf{a}_{\node, t})
% \geq 0, \quad t = 0, \dots, T.
\end{align}
We now explain the terms in expression \eqref{eq-objective} in detail.
The \emph{reward}/\emph{utility} function ${u}_{\node, t}$ is, in general,
time-varying.
That is to say, time-homogeneity which is required for standard recursive
models and steady-state approximation models is not a requirement in the present
model.
Under certainty, the state $\boldsymbol{\omega}_{\node, t} $ is simply the 
vector $\mathbf{K}_{\node, t}$ of capital endowments for each region and
sector.
The action or control variable $\mathbf{a}_{\node, t}$ is a quadruple
consisting of consumption $\mathbf{C}_{\node, t}$,
labour $\mathbf{L}_{\node, t}$,
intermediate input flows $\mathbf{M}_{\node, t}$, and
fixed capital formation or investment flows $\mathbf{N}_{\node, t}$.
Note that fixed capital formation includes, for instance, land reclaimed from
forest or sea.

We discuss these variables in more detail below.
As we will see, there are numerous other variables that are vital to the
model. 
In the language of nonlinear programming, the state-action pair
$(\boldsymbol{\omega}_{\node, t}, \mathbf{a}_{\node, t})$
are \emph{basic variables}.
To emphasize this property, we write all such variables in \textbf{bold} font.
In the present model, all variables that are not basic are \emph{intermediate}.
By basic we mean that the variable cannot be explicitly
written as a function of the other variables without recourse to
first-order or second-order conditions of the optimization (or equilibrium)
problem.

Turning now to the constraints, we begin with the transition laws/dynamics for
capital followed by the intratemporal feasibility constraints such as
production technology and market clearing.
%==============================================================================
\paragraph{Constraints at each node: transition dynamics}
%==============================================================================
Let $I$ and $J$ both denote the set of sectors and let $R$ denote the set of
regions.
We use $I$ to index the rows of an input-output matrix and $J$ to index the
columns.
Capital $\mathbf{K}_{\node, t}$ is a vector with values
$\nnreal ^ {J \times R}$ that evolves according to the standard equation for
each node $\node$ and time $t$.
Since the depreciation rate $0 < \delta_{j} < 1$ may vary across sectors, we
allow the following generalised forward difference operator $\Delta_j$ to vary
across sectors:
\[
  \Delta_{j}(\mathbf{K}_{j, \node, t})
    = \mathbf{K}_{j, \node, t + 1} -
      (1 - \delta_j) \cdot \mathbf{K}_{j, \node, t}. 
\]
For each $j, \node $ and $t$, the agent(s) are constrained in their ability to
modify the transition law for capital.
Letting $\Delta$ denote the function with component functions
$\Delta_{j}$, the transition law holds when the constraint function
\begin{equation}\label{eq-kapital}
  G_{\node, t}^{\,\textup{tran}}(
    \mathbf{a}_{\node, t}, \boldsymbol{\omega}_{\node, t}
  )
%    = (1 - \delta_{j}) \cdot
    = \textup{inv}_{\node, t}
      - \Delta_{}(\mathbf{K}_{\node, t})
%      - \delta_{j} \cdot \mathbf{K}_{r, \node, t}
\end{equation}
is equal to zero (in $\R^{J \times R}$).
We now break the formation of new capital down into its scalar components.
Letting the indices vary across their corresponding sets, investment
$\textup{inv}_{j, r, \node, t}$ is the scalar-valued variable defined by
aggregating over the \emph{basic} fixed capital formation vector
$\mathbf{N}_{j, r, \node, t} = (\mathbf{N}_{i,j, r, \node, t}: i \in I)$.
In particular the constant-elasticity-of-substitution (CES) aggregation
%We assume that fixed capital formation and investment are related via a
%standard constant elasticity of substitution (CES) function
\begin{equation}\label{eq-ces-inv}
  \textup{inv}_{j, r, \node, t} = A_{j, r}^{\textup{inv}}
    \cdot \left(\sum%_{i \, \in \, I}
      \left\{
      \acute \sigma_{i,j, r}^{\textup{inv}}
        \cdot \left( \mathbf{N}_{i,j, r, \node, t}\right)
        ^ {\,\rho_{j, r} ^ {\textup{inv}}}
      : i \in I\right\}
    \right) ^ {1 / \rho_{j, r}^{\textup{inv}}}.
%    \right) ^ {\kappa_{j, r}^{\textup{inv}} / \rho_{j, r}^{\textup{inv}}}.
\end{equation}

\begin{remark*}[on CES aggregators]
  CES functions are common in the present
  model. 
  Let $f_{j, r}^{\textup{inv}}$ denote the scalar-valued
  function mapping the fixed capital formation vector
  $ \mathbf{N}_{j, r, \node, t} \mapsto
  \textup{inv}_{j, r, \node, t} / A_{j, r}^{\textup{inv}}$
  and let $f^{\textup{inv}} = (f_{j, r}^{\textup{inv}}: j \in J, r \in R )$
  denote the corresponding vector-valued function that allows us to rewrite
  \eqref{eq-ces-inv} as
  \begin{equation}\label{eq-ces-concise}
  \textup{inv}_{\node, t} = A^{\textup{inv}}
    \circ f^{\textup{inv}}(\mathbf{N}_{\node, t}),
  \end{equation}
  where $\circ$ denotes the Hadamard (component-wise) product.
  The functional form \cref{eq-ces-concise} will allow us to emphasize that our
  corresponding aggregator $f^{\textup{lab}}$ for labour is homogenous across
  time whereas $A_{t}^{\textup{lab}}$ grows exponentially when technological
  progress is labour-augmenting.\footnote{
    Under certainty, $f^{\textup{inv}}$ is also homogenous across nodes.
  }
  In contrast, $A_{\node, t}^{\textup{inv}}$ evolves as agents learn.
\end{remark*}
The scalar $A_{j, r}^{\textup{inv}}$ is the productivity parameter that captures
how efficient agents are at transforming other goods into new type-$(j,r)$
capital.

Next, the scalar
\[
  \acute \sigma_{i,j, r}^{\textup{inv}}
  = \left( \sigma_{i,j, r}^{\textup{inv}} \right)
    ^ {1 / \varepsilon_{j, r}^{\textup{inv}}}
\]
%= (\hat{\sigma}_{i,j, r}^{\textup{inv}}) ^ {1 / \varepsilon_{j, r}^{\textup{inv}}}$
and where $\sigma^{\textup{inv}}$ is the vector in
$[0, 1]^{I \times J \times R}$ of normalised \emph{share} parameters with
elements representing the importance of good $i$ in forming $(j, r)$'s new
capital.
We derive $\sigma_{j, r}^{\textup{inv}}$ by taking a column of fixed capital
formation flows between sectors for region $r$ and normalising so that the
column sum is one.
In the language of input-output analysis, $\sigma_{r}^{\textup{inv}}$ is akin
to the matrix of technical coefficients for region $r$, but for new capital
formation instead of output.

As usual,
$\rho_{j, r}^{\textup{inv}} = 1 - 1 / \varepsilon_{j, r}^{\textup{inv}}$ is
decreasing in the elasticity of substitution $\varepsilon_{j, r}^{\textup{inv}}$
which lies in the range $ 0.1 $ to $0.9$.\footnote{
  Recall that CES aggregator is Leontief when its elasticity
  parameter tends to zero and ``linear'' (\ie\ Cobb--Douglas) when its
  elasticity parameter tends to one.
}
%Thus, $\rho_{j, r}^{\textup{inv}} $ lies in the range $-9$ to $-1.1$.
Finally, a necessary and sufficient condition
\citep[Theorem 2]{Kojic-Concavity} for strict concavity of the
components of $f^{\textup{inv}}$ is that the \emph{scale} parameter
$\kappa^\textup{inv}$ has elements strictly between $0$ and $1$.

We capture the inertia in adjusting capital
via the cost
\begin{equation}
  \textup{adj}_{j, r, \node, t}
    = A_{j, r} ^ \textup{adj} 
      \cdot \frac{(\mathbf{K}_{j, r, \node, t + 1}
      - \mathbf{K}_{j, r, \node, t})^2}
        {\mathbf{K}_{j, r, \node, t}}.
\end{equation}
A useful property of this adjustment cost is that it makes for more realistic
rates of convergence between regions.
It also allows us to capture the potential costs of closing down a
factory or mine and repurposing the land to other uses.
For each region and each sector, this cost is deducted from time $\node, t$
output which we describe below.
%==============================================================================
\paragraph{The production function}
%==============================================================================
In addition to capital, labour is the other \emph{primary factor of
production.}
Labour resources also vary across regions, sectors and time.
To simplify the exposition we avoid explicit constraints on labour supply and
instead have a disutility of labour supply similar to
\citet{Atalay-Sectoral_shocks} and \citet{CJ}.

Beyond primary factors, we have intermediate inputs.
As with investment, we specify intermediate inputs to production via a CES
aggregator $f^{\med}: \nnreal^{I \times J \times R} \rightarrow
\nnreal^{J \times R}$.
%\footnote{
%  The superscript in $f_{j, r}^{\cdot}$ indexes the variety of CES
%  aggregators in the model.
%  Each function $f_{j, r}^{\cdot}$ is characterised by a corresponding
%  set of parameters
%  $A_{j, r}^{\cdot}, (\sigma_{i,j, r}^{\cdot}: i \in J),
%  \rho_{j, r}^{\cdot}, \kappa_{j, r}^{\cdot} $ and
%  $\varepsilon_{j, r}^{\cdot}$ as per \cref{eq-ces-inv}.
%}
Thus,
\begin{equation}\label{eq-ces-int}
  \med_{\node, t} = A^{\med}
    \circ f^{\med}(
      \mathbf{M}_{\node, t}
      )
\end{equation}
where, for each $r, n$ and $t$, $\textbf{M}_{r, \node, t}$
corresponds to a standard input-output matrix of intermediate flows for a given
region.
%Long--Plosser flows (between sectors and across regions) do not distinguish
%between flows of goods such as flour or water that are sometimes viewed as
%purely intermediate and flows such as machinery that are sometimes viewed pure
%capital.
%In \citet{Long_Plosser-Real_business_cycles}, inventories of flour are a form
%of capital and current usage of machinery is an intermediate input.
%Although this assumption is not essential to the methodology, it is reasonable
%given the available data.
%For although standard input-output tables report investment as separate from
%intermediate inputs, this is on the basis of accounting as opposed to economic
%principles.
%Moreover, recent attempts \citep{Elazaar_Raymond-Network} to estimate
%production networks using micro data (BLADE and GST data) do not distinguish
%between these two categories of flows.
%It is worth pointing out that although intermediate production is well-modelled
%in most CGE models, savings and investment do not contribute to future 
%utility in any meaningful way \citep{Hosoe_et_al-CGE_textbook}.


For every $r$ in $R$, $i$ in $I$, let
$f_{i, r}^{\textup{out}}: \nnreal^{3} \rightarrow \nnreal$ be a standard
CES function of capital, labour and the intermediate-input bundle % and imports
respectively:
\begin{equation}
	\textup{out}_{\node, t}
    = A^{\textup{out}} \circ f ^ \textup{out}(
        \mathbf{K}_{\node, t},
        \mathbf{L}_{\node, t},
        \med_{\node, t},
%        \mathbf{Y}_{\node, t}
        )
\end{equation}
%\begin{equation}
%  f_{i, r, \node, t} ^ {\textup{out}} (x, y, z)
%	  = \left(
%      \alpha_{i, r} ^ {\textup\textup{out},x} \cdot x ^ {\,\rho^{\textup{out}}}
%      + \alpha_{i, r} ^ {\textup\textup{out},y} \cdot y ^ {\,\rho^{\textup{out}}}
%      + \alpha_{i, r} ^ {\textup\textup{out},z} \cdot z ^ {\,\rho^{\textup{out}}}
%    \right) ^ {\, \frac{\kappa ^ {\textup{out}}}{\rho^{\textup{out}}}}
%     .\footnote{
%        As in \cref{eq-ces-inv}, the scale parameter $\kappa$ is
%        approximately equal to, but less than one to ensure strict concavity.
%        The $\alpha_{i, r}$ capture the relative importance of capital and
%        labour in each region-sector pair.
%     }
%\end{equation}
%==============================================================================
\paragraph{Constraints at each node: market clearing}
%==============================================================================
In our canonical model, we assume free trade between regions.
Thus, there is a single market for each sector and markets clear for each
$i, \node, t $ in $I \times \nodes \times T$.
Market clearing holds the action vector $\mathbf{a}$ and state vector
$\boldsymbol{\omega}$ are such that the constraint function
\begin{equation}\label{eq-market-clearing}
  G^{\textup{clear}}(\mathbf{a}, \boldsymbol{\omega}) =
  \sum_{r \,\in\, R} \left\{
  \textup{out}_{r}
  - \mathbf{C}_{r}
  - \sum_{j \,\in\, J}
      \left\{\mathbf{N}_{j, r} + \mathbf{M}_{j, r}\right\}
%  - \mathbf{X}_{r}
  - \textup{adj}_{r} \right\}
\end{equation}
is equal to zero in $\R^{I \times  \nodes \times T}$.
%The market clearing property $G^{\textup{clear}}(\mathbf{a},
%\boldsymbol{\omega})=0$ then follows by virtue of the fact that
%the utility function $u_{\node, t}$ is strictly increasing in the
%consumption of every good at every $(\node, t)$.
%==============================================================================
\paragraph{Reward and Terminal Value functions at each node}
%==============================================================================
Beyond strictly increasing, for each region, node and time, we take the utility
function
\[
u_{r, \node, t}: \nnreal^{I \times J} \rightarrow \R, \quad
  (\mathbf{C}_{r, \node, t}, \mathbf{L}_{r, \node, t})
    \mapsto \textup{utl}_{r, \node, t},
\]
 to be strictly concave in consumption and labour.
To ensure 
Similar to both \citet{Atalay-Sectoral_shocks} and \citet{CJ}, we have
\begin{equation}
  \textup{utl}_{r, \node, t}
    = \beta^{t}
%%      \cdot \left(
%%        \sum_{i \,\in\, J} \sigma_{i, r}^\textup{utl}
%%          \cdot \mathbf{C}_{i, r, \node, t} ^ {\,\rho_{r}^\textup{utl}}
%%        \right) ^ {\kappa_{r}^{\textup{utl}} /\rho_{r}^{\textup{utl}}}
      \cdot \left(
        A_{r}^\textup{con}
          \cdot f_{r}^{\textup{con}}(\mathbf{C}_{i, r, \node, t}: i \in J)
        + A_{r}^{\textup{lab}}
          \cdot f_{r}^{{\textup{lab}}}(\mathbf{L}_{i, r, \node, t}: i \in J)
      \right)
\end{equation}
where $0 < \beta < 1$ is the parameter that describes how agents discount
the future.
The utility weight $A_{r}^{{\textup{lab}}}$ is distinguished by the property that
it is typically negative to reflect the disutility of labour.
To ensure concavity of utility in its arguments, a feature of the sectoral
aggregator for labour is that 
$\rho_{r}^{{\textup{lab}}} = 1 + 1 / \varepsilon_{r}^{{\textup{lab}}}$.
Thus, unlike every other $\rho$ parameters of the model, it is always
positive, greater than one, and decreasing in $\varepsilon_{r}^{\textup{lab}}$.
We recover the Atalay functional for the labour aggregate in utility by taking
$\varepsilon_{r}^{\textup{lab}} = \infty$ and taking the scale parameter
$\kappa_{r}^\textup{con} = 3$.
In this case, the labour aggregator is perfect substitutes in labour across
different sectors.
The condition for strict convexity of $f_{r}^{\textup{lab}}$
is that $\kappa_{r}^{\textup{lab}} > \rho_{r}^{\textup{lab}}$.
This, together with $0 <\kappa_{r}^{\textup{con}} < 1$, suffices for strict
concavity of $u_{r, \node, t}$.

The weighted sum across regions (with weights determined by population) of
regional reward functions yields the ``global''/``whole-of-economy'' reward
functions ${u}_{\node, t}$ of \eqref{eq-objective} for each node.
This, together with standard assumptions
\citep[Theorem 12.2.12]{Stachurski-Economic_dynamics}, is sufficient for strict
concavity of the terminal value function ${V}_{n + T}$ in the state
vector $\boldsymbol{\omega}_{\node, t}$.
Together with the structure we have imposed on the transition laws and
feasibility constraints, the social welfare Lagrangian $\mathfrak L_{\node}$ is
strictly concave function of the control vector $\mathbf{a}_{\node}$ for every
$\node$ in $\mc N$.
\footnote{
Recall that the Lagrangian is the sum of the objective and each of the
constraints scaled by the \emph{dual} variables (also known as Lagrange
multipliers).
}
\begin{remark*}
As usual, the Lagrange multipliers of market clearing constraints are also the
competitive market prices.
The AMPL language has features for constraining prices where necessary.
\end{remark*}
\paragraph{Well-posedness of the optimization problem at each node}
The above conditions ensure that the solving for the general equilibrium of the
economy is equivalent to solving a strictly concave program at each node.
This implies that the equilibrium at each node is unique.
The first-order (necessary) conditions for are also sufficient and we can
tackle the problem with local (nonlinear optimization) solvers such as Conopt
or Ipopt which are many times faster than global solvers such as Baron.
\section{The normalised Euler error}
We now derive the normalised Euler error for the example in
\cref{sec-certainty}.

\section{Uncertainty}
As in structural econometric modelling
\citep{Reiss_Wolak-Structural_econometrics} it is important to distinguish the
following kinds of uncertainty:
\begin{enumerate}[I]
   \item\label{aue} agent uncertainty about the economic environment;
   \item\label{aub} optimization errors / behaviour of economic agents;
   \item\label{rum} researcher measurement and approximation errors.
\end{enumerate}
%==============================================================================
\subsection{Type \ref{aue} uncertainty}
%==============================================================================
In MAIWAR, type \ref{aue} uncertainty is captured using a decision tree.
A decision tree $\mc T$ is a pair $(\nodes, \edges)$ of nodes and directed
arcs/edges connecting future nodes to preceding ones.
A \emph{node} $\node$ is where agents make decisions.
An \emph{edge} $\edge$ is a pair $(\node, \node^{\mplus})$
of nodes that are connected in the sense that $\node$ immediately
precedes $\node^{\mplus}$.
Uncertainty arises when there is more than one edge emanating from some node
$\node$.\footnote{
  Since $\mc T$ is a tree, for every
  $\textfrak{e} = (\node, \node^{\mplus}) \in \textfrak{E}$
  if $e' = (\node', \node^{\mplus}) \in \mc E$,
  then $\node' = \node$.
}
Uncertainty at node $\node$ unfolds along edge $e$ to reveal the subsequent
decision node.
A \emph{path} through time consists of a maximal sequence of connected nodes.
That is sequences that start at the (unique) root node $\mathfrak{0}$ of the
tree and end on a leaf (a node with no explicitly modelled future times).
Let $\mc P$ denote the set of paths.

\begin{example}[Irreversible risk]\label{eg-irreversible}
   In our canonical example, where the focus is on net-zero emissions in the
   year 2050, we identify the root with the current year, \eg~2022.
   A leaf is then a state of the world at time 2050.
   Each year between now and 2049, there is a small but positive probability of
   a one-off, permanent productivity shock to output.
   The following paths characterise the decision tree:
   \begin{enumerate}[1]
      \item the path where the shock happens between 2022 and 2023;
      \item the path where the shock happens between 2023 and 2024;
      \item[] \dots
      \item[28] the path where the shock happens between 2049 and 2050.
   \end{enumerate}
   Each path identifies a distinct state of the world.
   Although this stochastic process is as simple as flipping a coin
   each year and the resulting state space consists of just 28 paths,
   the irreversible nature of the shock yields a
   \emph{nonstationary} (\ie~time inhomogeneous) economy with a nonunique
   steady state.
\end{example}
a pair of variables consisting  of capital
$\mathbf{K}_{\node, t}$ and the productivity shock term
\begin{equation}
  \textup{shock}_{\node, t} = \left\{
    \begin{array}{ll}
       0.9 & \textup{if the shock has happened}\\
       1 & \textup{otherwise.}
    \end{array}\right.
\end{equation}
We return to discuss capital in more detail below.
The shock $\textup{shock}$ is therefore macroeconomic in the sense that
its effect is uniform across regions $R$ and sectors $J$ in the economy.
A more general shock will yield a larger state space and corresponding set of
paths.\footnote{
   This simplification is purely for expositional purposes.
   In our code, this and other parameters that appear below are allowed to vary 
   across regions and sectors.
}
The conditional expectation $\mbb{E}_{\node}$ is defined on the set of functions
that are measurable with respect to the information available to the agent at 
node $\node$.\footnote{
   All these functions are scalar-valued.
}
\paragraph{Constructing the empirical distribution on paths}
Let us first define the strong solution we obtain from the above node-specific
optimization problems.
For a given decision tree $\mc T = (\mc N, \mc E)$ and corresponding set of
paths $\mc P$, let
\[
   \textup{StrongSol}(\mc P) \defeq \left\{
      (\boldsymbol{\omega}^{*}_{p}, a^{*}_{p}) : p \in \mc P
   \right\}
\]
where the superscript $*$ denotes the optimal state-policy values
for each decision node $\node$ along a given path $p$.
Note that $\textup{StrongSol}(\mc P)$ only contains the realised initial
state-policy values since, for each $\node$, the look-ahead values
$\left\{(\boldsymbol{\omega}^{*}_{\node, t}, a^{*}_{\node, t}) : t > 0 \right\}$ are only used to
formulate the plan at node $\node$.
This notion of solution is strong in the sense that, assuming the model is
fully-specified, it tells us precisely what will happen once the economy
reaches a given node.
Moreovei, rn simple cases, such as example \ref{eg-irreversible} where there
are just $28$ paths, $\textup{StrongSol}(\mc P)$ can also be viewed as a
complete contingent plan of action and future capital.
Another sense in which the solution is strong when $\countof \mc P$
is small is that we are able to observe the entire sample space: we are able to
observe the true distribution of optimal paths through time.

\paragraph{Weak solutions}
Beyond simple examples such as \cref{eg-irreversible}, we can only hope to
partially observe the sample space.
The set of optimal solution paths that we have derived is then a subsample and,
since our collection of nodes is incomplete, we can no longer refer to our
solution as strong.
The only sense in which we can speak of a solution is in terms of the empirical
distribution over paths. 
From this we can estimate the expected path, the variance, and so on.
We now formalise this weaker notion of solution.

Let $\mathfrak{0}$ denote the root node of the decision tree.
Recalling that we assume the decision tree has paths of uniform length, let $S$
denote the set of \emph{path times}: for each $s$ in $S$ the node $p_s$ is $s$
steps into the future from $\mathfrak{0}$ along path $p$.
We begin by constructing the empirical distribution associated with
$\mathfrak{0} + s$ for each $s \in S$.
Let $D_s$ denote the set of nodes that are $s$ steps into the future from
$\mathfrak{0}$.\footnote{
   That is $N_{s} = \left\{ p_{s} \in \mc N : p \in \mc P\right\}$.
}

For any $s$ in $S$ note that $\mathbf{C}_{i, r, \mathfrak{0} + s}$ is a
scalar-valued random variable on $D_s$. 
Moreover, for each $\node$ in $D_s$, let $\textup{CON}_{i, r, n}$ denote a
realisation of this random variable.
By realisation, we mean a strong solution to this particular variable.
For any of our policy or state variables $x_{\mathfrak{0}+s}$ let $X_d$ denote
the realisation associated with node $\node$.
Then the empirical distribution generated by $\left\{X_{d'}: d' \in D_s\right\}$ is
the function
$F_{x_{s}} : \R \rightarrow [0, 1]$,
\begin{equation}\label{eq-empirical-dbn}
   F_{x_s}(z) = \frac{1}{\countof N_{s}}
   \cdot \sum_{n \,\in\, N_{s}} 1_{X_d \,\leq\, z}.
\end{equation}

The weak solution for a given policy or state variable $x$ is simply the
sequence $F_{x} = \left\{F_{x_{s}}: s \in S\right\}$ of empirical distributions.
$F_{x}$ is the empirical distribution of the path-valued random variable $x$.
For instance, $F_{\mathbf{C}_{i, r}}$ is the empirical distribution for the
path of consumption of sector-$i$ goods in region $r$.

In turn, the weak solution for the entire problem is the joint empirical
distribution $F_{\mathfrak{0}}$ we obtain by taking the product of the
marginals.
(This is the case because all these probability measures are push-forwards that
are dependent on the same source of uncertainty.)
\paragraph{Measuring accuracy in terms of the Euler error}
One normative measure of the accuracy of our weak solutions is the Euler error
that is associated with the (intertemporal) Euler equation.
This equation is a necessary condition for optimality of the stochastic dynamic
program \citet{Stokey_Lucas-Recursive}.
In the present setting the derivation of Euler equation is complicated by two
factors: the number of control variables exceeds the number of state variables;
and there is no explicit form for
To address this issue, for each $\node$, we appeal to the envelope theorem.
In particular, we follow \citet{Gonzalez_Hernandez-Euler}
and derive an indirect utility function
$\textup{v}_{\node}$ as a function of current and next-period states $\boldsymbol{\omega}_{\node}$
and $\boldsymbol{\omega}_{\node^{\mplus}}$.
\begin{align*}
   \textup{v}_{\node}(\boldsymbol{\omega}_{\node}, \boldsymbol{\omega}_{\node^{\mplus}}) \defeq
   \max_{\mathbf{a}_{\node}}  \quad {u}_{\node}(\boldsymbol{\omega}_{\node}, \mathbf{a}_{\node}) \quad &
     \textup{subject to}\\
   \quad \boldsymbol{\omega}_{\node^{\mplus}} = \textup{tran}_{\node}(\boldsymbol{\omega}_{\node}, a_{\node}) \quad
   \textup{and} & \quad \textup{feas}_{\node}(\boldsymbol{\omega}_{\node}, \mathbf{a}_{\node}) \geq 0
\end{align*}
where the transition and feasibility constraints are those we have described
above.
%As a matter of expedience, we follow the AMPL convention of identifying
%Lagrange multipliers with their corresponding constraint nomenclature.
%Thus $\textup{tran}_{s}$ and $\textup{feas}_{s}$ will denote the (random)
%Lagrange multipliers $s$ steps into the future from the root node
%$\mathfrak{0}$.
Then, via the envelope theorem, for models with exogenous uncertainty (where
actions do not affect transition probabilities), the Euler error associated
with step $s$ is
\begin{equation}
\frac{
  \beta \cdot \mathbb{E}_{\mathfrak{0}} \left\{
    \nabla_{\mathbf{K}_{s+1}} \textup{v}_{s+1}(\boldsymbol{\omega}_{s+1}, \boldsymbol{\omega}_{s+2})
  \right\}
  }{\mathbb{E}_{\mathfrak{0}} \left\{
    \nabla_{\mathbf{K}_{s+1}} \textup{v}_{s}(\boldsymbol{\omega}_{s}, \boldsymbol{\omega}_{s+1})
  \right\}
  } + 1
\end{equation}
It requires that the marginal loss (the denominator is negative) associated
with foregoing today's consumption (by increasing tomorrow's capital) is
balanced against the marginal benefit of increasing consumption tomorrow.
Note that the expectation $\mbb E_{\mathfrak{0}}$ in the expression for
Euler error is the empirical one we have derived as part of our weak solution.
In other words, our condition for accuracy demands that the empirical Euler
error is small.
As \citet{CJ} show for a wide variety of settings, the worst case of this error
across $s$ in $S$ is less than $1\%$.

We note that, since there is no explicit form for the indirect utility function
$\textup{v}$, we build on \citet{Scheidegger_Bilionis-Gaussian_processes} to
use gaussian processes to generate an approximation.
That is, we introduce uncertainty of type \ref{rum}.
This brings us to the next section.
%==============================================================================
\subsection{Types \ref{aub} and \ref{rum} uncertainty: approximations}
\label{sec-approximation}
%==============================================================================
In MAIWAR, weak solutions allow us to accommodate uncertainty of type \ref{aub}
and \ref{rum} without sacrificing theoretically appealing
solution properties such as the \emph{intertemporal Euler equation} a feature
that is absent from CGE models: see \cite{Dixon_Rimmer-Euler_CGE} for a recent
attempt to introduce this concept into the CoPS methodology.\footnote{
   This paper provided inspiration for our efforts.
}
Our agents (and modellers) strive to ensure that the marginal benefits of
current consumption against the marginal benefits of uncertain future
production.

Yet in order to arrive at a tractable and fast method for deriving a weak
solution, we will need to approximate.
By this we do not only mean the inevitable partial sampling of the set of
paths we have already highighted in the previous section, we mean:
\begin{enumerate}
\item approximating the terminal value function (see 
\citet{Bertsekas-Reinforcement_learning} for a detailed exposition of this
matter in the setting of reinforcement learning);
\item  by using certainty-equivalent projections (see \citet{CJ} and
   \citet{CJS-Certainty_equivalent} for a detailed exposition of this concept).
\end{enumerate}

Our first departure from canonical expected utility maximisation, where agents
form a complete contingent plans of action, is as follows.
At each decision node, agents solve a nonlinear program where errors
associated with future type \ref{aue} uncertain variables are replaced with
some convex combination of the best and worst case scenario.
Abundant evidence from the literature on behavioural economics shows
that agents adopt similar heuristic approaches to dealing with complex
decisions in the face of uncertainty.
The ``\emph{certainty equivalent}'' projection of \cite{CJ} is one such
heuristic. 
Aside from tractability, the inbuilt flexibility and scope for improvement in
aspect of the model is one reason for adopting the methodology of \cite{CJ}.
That is say, we view this, not only as an approximation, but also a reasonable
behavioural feature.

A second form of type \ref{aub} uncertainty arises because economic agents
struggle with long time horizons: indeed, isn't this why policy makers appeal 
to CGE models in the first place?
Evidence shows that agents focus their attention on the next few periods.
This motivates the finite look-ahead feature of our model.
Our agents focus on the near future as opposed to the far-flung steady states
at the heart of other approaches.

In MAIWAR there is a useful alignment between modeller's interests and the type
\ref{aub} uncertainties we have just outlined.
For although there are well-established methods for implementing large-scale
linear stochastic programs, the tools for large-scale nonlinear stochastic
programming are less evolved \citet{Rehfeldt-PETSc}.
Furthermore, although the time horizon of our agents is indefinite (and
typically modelled as infinite), most methods for dealing with such time
horizons assume stationarity or provide a linear approximation around the
nonstochastic steady state.
Nor is there an obvious way to model 2050 net-zero emissions without proper
modelling of the medium term.
Constraints that will bind in 28 years time, but have implications today are
poorly suited to steady-state approximations and standard policy/value
function iteration methods.
\footnote{
   In our experience, "global" value and policy function iteration methods
   are also slow and unstable in the setting of multi-sectoral models with
   medium-term constraints.
   See \citet{CJ} for a related discussion.
}

In summary, our agents solve finite sequences of nonlinear programs each with a
finite horizon and an approximation for the terminal value function. 
This is an approximation in the sense that normative agents solve nonlinear
stochastic programs each with an infinite horizon or the true terminal value
function.
We argue that our approximations are well-aligned with behaviour or agents in
practice.
Moreover, the weak solution that we derive explicitly accommodates our
approximation methods.
Finally, since this weak solution is an empirical distribution we can verify 
that the solution is accurate to within a $1\%$ Euler error approximation. 
The same measure is typical of ``global approaches''.
For instance, in the deep neural network approach of
\citet{Scheidegger_et_al-DEQN} the policy iteration process is successful once
the Euler error term converges to a sufficiently small number.
Our investigations of this approach yielded slow and unstable results.
The notion of a weak solution recognizes the fact that our modeller is
typically facing substantial type \ref{rum} uncertainty.
Parametric uncertainty is a feature of the model and a central part of the
solution.

\subsection{Computation and software}

The uncertainty we model is facilitated and expedited by the fact that we have
in place the software infrastructure to ensure that simple model sweeps are
easy to conduct on the local HPC cluster.
The unit for parallelisation is paths: each path is passed to a single core.
This means that most desktop computers can easily parallelise.
For example, consider our canonical example of irreversible risk: where there
are 28 paths (each of length 28 years), and assume 7 regions and 20 sectors.
For the functions we specify above, each step along a path takes $20$ seconds
on average.
Thus a path takes $28\times \frac{1}{3} \approx 9$ minutes to solve.
For a desktop with 4 available cores, we are able to assign 7 paths to each
core and complete the generate the full empirical distribution in
$7\times 9 \approx 1$ hour.

After an exhaustive search, we chose AMPL (A modelling language for
mathematical programming) \citet{Fourer_Gay_Kernighan-AMPL} as the language and 
ecosystem for communicating with solvers.
AMPL is used for pre-processing/pre-solving: the stage before the nonlinear
optimization problem is passed to the solver.
The goal of pre-solving is to ensure the solver stage is faster and more
reliable.
Pre-solving includes the generation of automatic, sparse derivatives as well as
domain/variable reduction.
AMPL syntax is much closer to standard mathematics relative to GAMS and we have
found it to produce faster total solve times than GAMS.\footnote{
\citet{CJ} find that GAMS is substantially faster than Matlab.
}
We spent a lot of time trying purely open source options in Python, Julia and
to lesser extent C++.
This experimentation also involved machine learning tools (based on
Scheidegger's work).
We found that, overall, combining the \citet{CJ} methodology with AMPL
pre-solving yields unrivalled modelling performance, flexibility and
reliability.


We have found three nonlinear solvers to be most useful.
Conopt which is proprietary and based on the reduced-gradient method (itself
based on the robust simplex method).
The open source Ipopt (pronounced ``eye-pea-opt"). This is celebrated
application of the interior point by \citet{Wachter_Biegler-Ipopt}.
Knitro which is tries a number of approaches and, unlike Conopt and Ipopt can
handle integer-valued variables. 
We have tried other solvers as well, but the above are outstanding, with Conopt
being the most robust.

Future improvements to our methodology relate to tuning MAIWAR model
parameters and improving on our existing framework for parallel computing.
In particular, to enable simple automated parallelisation even desktop
computers, we will use the Python API for AMPL along with nextflow.
Currently this process is manual and automatic tuning of parameters using
machine learning is  step in the development of MAIWAR.
Many of the tools that we worked on in the second half of last year (first six
months of SMaRT) will serve us well in this respect.

\subsection{Nonlinearity and (non)steady-state methods}

\citet{Atalay-Sectoral_shocks} estimates to be approximately $0.1$ so that
$\rho \approx -9$.
\\
Such approximations are even less appealing for nonlinear production networks
that arise when elasticities of substition are below one.
\\
Impressive recent work using Hamiltonian methods still fails to capture
the central economic theme of uncertainty.


\bibliographystyle{apalike} \bibliography{references}
\end{document}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
